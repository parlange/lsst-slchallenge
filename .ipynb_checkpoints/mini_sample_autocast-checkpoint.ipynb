{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475abd57-899b-4f10-8a60-1c9597fb55fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision timm astropy scikit-learn tqdm seaborn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2768f0c-31a5-4cf8-854e-c9875484d261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from astropy.io import fits\n",
    "import pandas as pd\n",
    "\n",
    "# Define paths\n",
    "dataset_path = '/home/parlange/datasets/mini_sample/images'\n",
    "metadata_path = '/home/parlange/datasets/mini_sample/'\n",
    "\n",
    "# Function to analyze FITS files\n",
    "def analyze_fits_files(directory):\n",
    "    fits_data = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.fits'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    with fits.open(file_path) as hdul:\n",
    "                        header = hdul[0].header\n",
    "                        fits_data.append({\n",
    "                            'File': file_path,\n",
    "                            'Dimensions': hdul[0].data.shape if hdul[0].data is not None else None,\n",
    "                            'Bitpix': header.get('BITPIX'),\n",
    "                            'NAXIS': header.get('NAXIS'),\n",
    "                            'Object': header.get('OBJECT', 'Unknown'),\n",
    "                            'Instrument': header.get('INSTRUME', 'Unknown'),\n",
    "                            'Date': header.get('DATE-OBS', 'Unknown')\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file_path}: {e}\")\n",
    "    return fits_data\n",
    "\n",
    "# Analyze FITS files in both directories\n",
    "images_fits_data = analyze_fits_files(dataset_path)\n",
    "metadata_fits_data = analyze_fits_files(metadata_path)\n",
    "\n",
    "# Combine results and create a DataFrame\n",
    "fits_analysis_df = pd.DataFrame(images_fits_data)\n",
    "metadata_analysis_df = pd.DataFrame(metadata_fits_data)\n",
    "\n",
    "# Save the analysis results to a CSV file\n",
    "output_csv_path = 'fits_files_analysis.csv'\n",
    "fits_analysis_df.to_csv(output_csv_path, index=False)\n",
    "metadata_analysis_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "fits_analysis_df\n",
    "metadata_analysis_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fd516d-b03e-499f-82e7-60f71a4bf946",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Images DataFrame:\")\n",
    "print(fits_analysis_df['File'].head())\n",
    "\n",
    "print(\"\\nMetadata DataFrame:\")\n",
    "print(metadata_analysis_df['File'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa36131-69fd-4e89-97cf-1c45029ee604",
   "metadata": {},
   "outputs": [],
   "source": [
    "fits.info('/home/parlange/datasets/mini_sample/metadata_catalog.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f919027-0812-4110-bf62-aad116b23437",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdul = fits.open('/home/parlange/datasets/mini_sample/metadata_catalog.fits')\n",
    "print(hdul[1].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b0e792-39c0-4687-93b4-b4bcf9c1c5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fits.info('/home/parlange/datasets/mini_sample/images/image_catalog_g.fits')\n",
    "fits.info('/home/parlange/datasets/mini_sample/images/image_catalog_r.fits')\n",
    "fits.info('/home/parlange/datasets/mini_sample/images/image_catalog_i.fits')\n",
    "fits.info('/home/parlange/datasets/mini_sample/images/image_catalog_z.fits')\n",
    "fits.info('/home/parlange/datasets/mini_sample/images/image_catalog_y.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2af0b7-968c-4edc-8c3d-28443f116248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FITS file\n",
    "fits_file = '/home/parlange/datasets/mini_sample/metadata_catalog.fits'\n",
    "with fits.open(fits_file) as hdul:\n",
    "    # Access the table data\n",
    "    data = hdul[1].data\n",
    "\n",
    "# Convert all columns to a pandas DataFrame\n",
    "columns = data.columns.names  # Get all column names\n",
    "data_dict = {col: data[col] for col in columns}  # Extract data for all columns\n",
    "df = pd.DataFrame(data_dict)\n",
    "\n",
    "# Save all parameters to a CSV file\n",
    "output_csv = '/home/parlange/datasets/mini_sample/labels_parameters.csv'\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"All parameters extracted and saved as '{output_csv}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41960bf9-936c-447f-911f-b14ea2c26bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1888c1f-d3f5-4e1c-bb50-747b85cd0e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape for g band: (20000, 61, 61)\n",
      "Shape for r band: (20000, 61, 61)\n",
      "Shape for i band: (20000, 61, 61)\n",
      "Shape for z band: (20000, 61, 61)\n",
      "Shape for y band: (20000, 61, 61)\n",
      "Final X shape: (20000, 5, 61, 61)\n",
      "y shape: (20000,)\n",
      "Per-channel means: tensor([0.2430, 0.3049, 0.5290, 0.6589, 0.7684])\n",
      "Per-channel stds: tensor([3.4988, 3.4331, 6.2919, 7.4187, 7.9995])\n",
      "Using device: cuda\n",
      "Number of GPUs: 2\n",
      "\n",
      "==== Training Model: Swin ====\n",
      "\n",
      "\n",
      "=== Fold 1/5 for model Swin ===\n",
      "Train indices length: 16000, Val indices length: 4000\n",
      "Fine-tuning mode: classification_head\n",
      "Model wrapped in DataParallel.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "poch 1/2: 100%|██████████████████████████████████████████████████| 125/125 [00:30<00:00,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Train Loss: 0.6988, Train Acc: 52.72% | Val Loss: 0.6767, Val Acc: 68.10%\n",
      "Checkpoint saved at checkpoints/model_Swin_fold_1_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "poch 2/2: 100%|██████████████████████████████████████████████████| 125/125 [00:30<00:00,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 - Train Loss: 0.6882, Train Acc: 55.33% | Val Loss: 0.6666, Val Acc: 70.62%\n",
      "Checkpoint saved at checkpoints/model_Swin_fold_1_best.pth\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ClassifierHead' object has no attribute 'out_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 933\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel wrapped in DataParallel.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 933\u001b[0m     history, all_labels_np, all_preds_np, all_preds_probs, auc_roc, f1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_fold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    936\u001b[0m     fold_results\u001b[38;5;241m.\u001b[39mappend((auc_roc, f1))\n\u001b[1;32m    937\u001b[0m     model_metrics[model_name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauc_roc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(auc_roc)\n",
      "Cell \u001b[0;32mIn[22], line 416\u001b[0m, in \u001b[0;36mtrain_one_fold\u001b[0;34m(model, train_loader, val_loader, device, num_epochs, patience, fold, model_name)\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# After training, compute metrics\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# Assuming multi-class classification\u001b[39;00m\n\u001b[0;32m--> 416\u001b[0m num_classes_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_features\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m model\u001b[38;5;241m.\u001b[39mhead\u001b[38;5;241m.\u001b[39mout_features\n\u001b[1;32m    417\u001b[0m all_labels_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(all_labels)\n\u001b[1;32m    418\u001b[0m all_preds_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(all_preds)\n",
      "File \u001b[0;32m~/anaconda3/envs/vit/lib/python3.9/site-packages/torch/nn/modules/module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ClassifierHead' object has no attribute 'out_features'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import timm\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import RandomApply\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "\n",
    "import pandas as pd  # For DataFrame handling\n",
    "from IPython.display import display  # For displaying DataFrames in Jupyter\n",
    "\n",
    "# Suppress potential warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler  # Added for Mixed Precision\n",
    "\n",
    "\n",
    "def initialize_model(model_name, num_classes, pretrained=False, stochastic_depth_prob=0.1, \n",
    "                    fine_tune_mode=\"classification_head\", last_n=6):\n",
    "    \"\"\"\n",
    "    Initialize a model from the given model_name without ImageNet pretraining and\n",
    "    allow for a custom number of input channels (5).\n",
    "\n",
    "    Args:\n",
    "        model_name (str): One of [ViT, CaiT, DeiT, DeiT3, Swin, Twins_SVT, Twins_PCPVT, PiT, MLP-Mixer].\n",
    "        num_classes (int): Number of output classes.\n",
    "        pretrained (bool): Whether to load a pretrained model. Set to False to avoid mismatches in input channels.\n",
    "        stochastic_depth_prob (float): The stochastic depth probability for supported models.\n",
    "        fine_tune_mode (str): 'classification_head', 'last_n_blocks', 'half', or 'all_blocks'.\n",
    "        last_n (int): Number of blocks/layers to unfreeze if using 'last_n_blocks'.\n",
    "\n",
    "    Returns:\n",
    "        model: The initialized and partially unfrozen model.\n",
    "    \"\"\"\n",
    "    def freeze_all(model):\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "def initialize_model(model_name, num_classes, pretrained=False, stochastic_depth_prob=0.1, \n",
    "                    fine_tune_mode=\"classification_head\", last_n=6):\n",
    "    \"\"\"\n",
    "    Initialize a model from the given model_name without ImageNet pretraining and\n",
    "    allow for a custom number of input channels (5).\n",
    "\n",
    "    Args:\n",
    "        model_name (str): One of [ViT, CaiT, DeiT, DeiT3, Swin, Twins_SVT, Twins_PCPVT, PiT, MLP-Mixer].\n",
    "        num_classes (int): Number of output classes.\n",
    "        pretrained (bool): Whether to load a pretrained model. Set to False to avoid mismatches in input channels.\n",
    "        stochastic_depth_prob (float): The stochastic depth probability for supported models.\n",
    "        fine_tune_mode (str): 'classification_head', 'last_n_blocks', 'half', or 'all_blocks'.\n",
    "        last_n (int): Number of blocks/layers to unfreeze if using 'last_n_blocks'.\n",
    "\n",
    "    Returns:\n",
    "        model: The initialized and partially unfrozen model.\n",
    "    \"\"\"\n",
    "    def freeze_all(model):\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def unfreeze_head(model, model_name):\n",
    "        \"\"\"\n",
    "        Unfreeze the classification head based on the model architecture.\n",
    "        \n",
    "        Args:\n",
    "            model: The PyTorch model.\n",
    "            model_name (str): Name of the model.\n",
    "        \"\"\"\n",
    "        head_found = False\n",
    "        if model_name in ['ViT', 'CaiT', 'DeiT', 'DeiT3', 'PiT', 'MLP-Mixer']:\n",
    "            # These models typically have a 'head' attribute\n",
    "            if hasattr(model, 'head'):\n",
    "                for param in model.head.parameters():\n",
    "                    param.requires_grad = True\n",
    "                head_found = True\n",
    "        elif model_name in ['Swin', 'Twins_SVT', 'Twins_PCPVT']:\n",
    "            # Swin and Twins models might have different head names\n",
    "            if hasattr(model, 'head'):\n",
    "                for param in model.head.parameters():\n",
    "                    param.requires_grad = True\n",
    "                head_found = True\n",
    "            elif hasattr(model, 'classifier'):\n",
    "                for param in model.classifier.parameters():\n",
    "                    param.requires_grad = True\n",
    "                head_found = True\n",
    "        \n",
    "        if not head_found:\n",
    "            print(f\"Warning: No identifiable classification head found for model {model_name}.\")\n",
    "\n",
    "    def unfreeze_last_n_blocks(blocks, n):\n",
    "        if n > len(blocks):\n",
    "            n = len(blocks)\n",
    "        for block in blocks[-n:]:\n",
    "            for p in block.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "    def unfreeze_half_blocks(blocks):\n",
    "        n = len(blocks) // 2\n",
    "        unfreeze_last_n_blocks(blocks, n)\n",
    "\n",
    "    def unfreeze_all_blocks(blocks):\n",
    "        for block in blocks:\n",
    "            for p in block.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "    # Initialize the model with 5 input channels and no pretrained weights\n",
    "    if model_name == 'ViT':\n",
    "        model = timm.create_model('vit_base_patch16_224',\n",
    "                                  pretrained=pretrained, \n",
    "                                  in_chans=5,\n",
    "                                  drop_path_rate=stochastic_depth_prob)\n",
    "        # Modify the classification head\n",
    "        model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "        ft_attr = 'blocks'\n",
    "    elif model_name == 'CaiT':\n",
    "        model = timm.create_model('cait_s24_224',\n",
    "                                  pretrained=pretrained, \n",
    "                                  in_chans=5,\n",
    "                                  drop_path_rate=stochastic_depth_prob)\n",
    "        model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "        ft_attr = 'blocks'\n",
    "    elif model_name == 'DeiT':\n",
    "        model = timm.create_model('deit_base_patch16_224',\n",
    "                                  pretrained=pretrained, \n",
    "                                  in_chans=5,\n",
    "                                  drop_path_rate=stochastic_depth_prob)\n",
    "        model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "        ft_attr = 'blocks'\n",
    "    elif model_name == 'DeiT3':\n",
    "        model = timm.create_model('deit3_base_patch16_224',\n",
    "                                  pretrained=pretrained,\n",
    "                                  in_chans=5,\n",
    "                                  drop_path_rate=stochastic_depth_prob)\n",
    "        model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "        ft_attr = 'blocks'\n",
    "    elif model_name == 'Swin':\n",
    "        model = timm.create_model('swin_base_patch4_window7_224',\n",
    "                                  pretrained=pretrained,\n",
    "                                  in_chans=5,\n",
    "                                  drop_path_rate=stochastic_depth_prob,\n",
    "                                  num_classes=num_classes)\n",
    "        ft_attr = 'layers'\n",
    "    elif model_name == 'Twins_SVT':\n",
    "        model = timm.create_model('twins_svt_base',\n",
    "                                  pretrained=pretrained, \n",
    "                                  in_chans=5,\n",
    "                                  drop_path_rate=stochastic_depth_prob)\n",
    "        model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "        ft_attr = 'stages' if hasattr(model, 'stages') else None\n",
    "    elif model_name == 'Twins_PCPVT':\n",
    "        model = timm.create_model('twins_pcpvt_base',\n",
    "                                  pretrained=pretrained, \n",
    "                                  in_chans=5,\n",
    "                                  drop_path_rate=stochastic_depth_prob)\n",
    "        model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "        ft_attr = 'stages' if hasattr(model, 'stages') else None\n",
    "    elif model_name == 'PiT':\n",
    "        model = timm.create_model('pit_b_224',\n",
    "                                  pretrained=pretrained, \n",
    "                                  in_chans=5,\n",
    "                                  drop_path_rate=stochastic_depth_prob)\n",
    "        model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "        ft_attr = 'blocks'\n",
    "    elif model_name == 'MLP-Mixer':\n",
    "        model = timm.create_model('mixer_b16_224',\n",
    "                                  pretrained=pretrained, \n",
    "                                  in_chans=5,\n",
    "                                  drop_path_rate=stochastic_depth_prob)\n",
    "        model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "        ft_attr = 'blocks'\n",
    "    else:\n",
    "        raise ValueError(f\"Model {model_name} not recognized.\")\n",
    "\n",
    "    # Freeze all layers\n",
    "    freeze_all(model)\n",
    "    # Unfreeze the classification head\n",
    "    unfreeze_head(model, model_name)\n",
    "\n",
    "    if fine_tune_mode == \"classification_head\":\n",
    "        return model\n",
    "\n",
    "    # Unfreeze additional layers based on the fine-tune mode\n",
    "    if fine_tune_mode == \"all_blocks\":\n",
    "        if ft_attr and hasattr(model, ft_attr):\n",
    "            blocks = getattr(model, ft_attr)\n",
    "            unfreeze_all_blocks(blocks)\n",
    "        else:\n",
    "            # Fallback: parameter-based unfreezing\n",
    "            for n, p in model.named_parameters():\n",
    "                if not p.requires_grad:\n",
    "                    p.requires_grad = True\n",
    "    elif fine_tune_mode == \"last_n_blocks\":\n",
    "        if ft_attr and hasattr(model, ft_attr):\n",
    "            blocks = getattr(model, ft_attr)\n",
    "            unfreeze_last_n_blocks(blocks, last_n)\n",
    "        else:\n",
    "            # Fallback: parameter-based unfreezing\n",
    "            all_params = list(model.named_parameters())\n",
    "            frozen = [(n, p) for (n, p) in all_params if not p.requires_grad]\n",
    "            if last_n > len(frozen):\n",
    "                last_n = len(frozen)\n",
    "            for (n, p) in frozen[-last_n:]:\n",
    "                p.requires_grad = True\n",
    "    elif fine_tune_mode == \"half\":\n",
    "        if ft_attr and hasattr(model, ft_attr):\n",
    "            blocks = getattr(model, ft_attr)\n",
    "            unfreeze_half_blocks(blocks)\n",
    "        else:\n",
    "            # Fallback: parameter-based unfreezing\n",
    "            all_params = list(model.named_parameters())\n",
    "            frozen = [(n, p) for (n, p) in all_params if not p.requires_grad]\n",
    "            half_n = len(frozen) // 2\n",
    "            for (n, p) in frozen[-half_n:]:\n",
    "                p.requires_grad = True\n",
    "    else:\n",
    "        raise ValueError(\"fine_tune_mode must be one of: 'classification_head', 'last_n_blocks', 'half', 'all_blocks'\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, val_loss, fold, model_name, checkpoints_dir=\"checkpoints\"):\n",
    "    \"\"\"\n",
    "    Save the model checkpoint.\n",
    "\n",
    "    Args:\n",
    "        model: The model to save.\n",
    "        optimizer: The optimizer state.\n",
    "        epoch (int): Current epoch number.\n",
    "        val_loss (float): Validation loss.\n",
    "        fold (int): Current fold number.\n",
    "        model_name (str): Name of the model.\n",
    "        checkpoints_dir (str): Directory to save checkpoints.\n",
    "    \"\"\"\n",
    "    os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    checkpoint_path = os.path.join(checkpoints_dir, f\"model_{model_name}_fold_{fold+1}_best.pth\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model_to_save.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_loss': val_loss\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "\n",
    "def train_one_fold(model, train_loader, val_loader, device, num_epochs=2, patience=1, fold=0, model_name=\"model\"):\n",
    "    \"\"\"\n",
    "    Train the model for one fold using Mixed Precision.\n",
    "\n",
    "    Args:\n",
    "        model: The model to train.\n",
    "        train_loader: DataLoader for training data.\n",
    "        val_loader: DataLoader for validation data.\n",
    "        device: Device to train on.\n",
    "        num_epochs (int): Maximum number of epochs.\n",
    "        patience (int): Early stopping patience.\n",
    "        fold (int): Current fold number.\n",
    "        model_name (str): Name of the model.\n",
    "\n",
    "    Returns:\n",
    "        history (dict): Training and validation loss and accuracy history.\n",
    "        all_labels_np (np.ndarray): True labels.\n",
    "        all_preds_np (np.ndarray): Predicted labels.\n",
    "        all_preds_probs (np.ndarray): Predicted probabilities.\n",
    "        auc_roc (float or dict): ROC AUC score. Float for binary, dict for multi-class.\n",
    "        f1 (float): F1 score.\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-3)  # Reduced lr and wd\n",
    "    scaler = GradScaler()  # Initialize GradScaler for mixed precision\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # For storing metrics\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "\n",
    "        # Progress bar for training\n",
    "        train_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True)\n",
    "        for X_batch, y_batch in train_iter:\n",
    "            X_batch = X_batch.to(device, non_blocking=True)\n",
    "            y_batch = y_batch.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():  # Enable autocast for mixed precision\n",
    "                outputs = model(X_batch)\n",
    "                \n",
    "                # Check outputs for NaN or Inf\n",
    "                if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
    "                    print(\"Model outputs contain NaN or Inf. Skipping this batch.\")\n",
    "                    continue  # Skip to the next batch\n",
    "\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                \n",
    "                # Check loss for NaN or Inf\n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    print(\"Loss is NaN or Inf. Skipping this batch.\")\n",
    "                    continue  # Skip to the next batch\n",
    "\n",
    "            scaler.scale(loss).backward()  # Scale the loss and backpropagate\n",
    "\n",
    "            # Check gradients for NaN or Inf\n",
    "            invalid_grad = False\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n",
    "                        print(f\"Gradient issue in parameter: {name}. Skipping optimizer step.\")\n",
    "                        invalid_grad = True\n",
    "                        break\n",
    "\n",
    "            if invalid_grad:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)  # Attempt to step optimizer to reset scaler\n",
    "                scaler.update()\n",
    "                continue  # Skip to the next batch\n",
    "\n",
    "            # Clip gradients\n",
    "            scaler.unscale_(optimizer)  # Unscale gradients before clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            scaler.step(optimizer)  # Perform optimizer step\n",
    "            scaler.update()         # Update the scaler for next iteration\n",
    "\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct_train += (preds == y_batch).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_acc = correct_train / len(train_loader.dataset)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        preds_list = []\n",
    "        labels_list = []\n",
    "        probs_list = []\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val = X_val.to(device, non_blocking=True)\n",
    "                y_val = y_val.to(device, non_blocking=True)\n",
    "                with autocast():  # Enable autocast during evaluation\n",
    "                    outputs = model(X_val)\n",
    "                    \n",
    "                    # Check outputs for NaN or Inf\n",
    "                    if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
    "                        print(\"Validation model outputs contain NaN or Inf. Skipping this batch.\")\n",
    "                        continue  # Skip to the next batch\n",
    "\n",
    "                    loss = criterion(outputs, y_val)\n",
    "                    \n",
    "                    # Check loss for NaN or Inf\n",
    "                    if torch.isnan(loss) or torch.isinf(loss):\n",
    "                        print(\"Validation loss is NaN or Inf. Skipping this batch.\")\n",
    "                        continue  # Skip to the next batch\n",
    "\n",
    "                val_loss += loss.item() * X_val.size(0)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct_val += (preds == y_val).sum().item()\n",
    "\n",
    "                preds_list.extend(preds.cpu().numpy())\n",
    "                labels_list.extend(y_val.cpu().numpy())\n",
    "\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                probs_list.extend(probs.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = correct_val / len(val_loader.dataset)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        all_preds.extend(preds_list)\n",
    "        all_labels.extend(labels_list)\n",
    "        all_probs.extend(probs_list)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc*100:.2f}%\")\n",
    "\n",
    "        # Checkpoint if best\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            save_checkpoint(model, optimizer, epoch, val_loss, fold, model_name)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # After training, compute metrics\n",
    "    # Assuming multi-class classification\n",
    "    num_classes_model = model.module.head.out_features if hasattr(model, 'module') else model.head.out_features\n",
    "    all_labels_np = np.array(all_labels)\n",
    "    all_preds_np = np.array(all_preds)\n",
    "    all_probs_np = np.array(all_probs)\n",
    "\n",
    "    # Binarize the labels for ROC AUC\n",
    "    all_labels_binarized = label_binarize(all_labels_np, classes=np.arange(num_classes_model))\n",
    "    if all_labels_binarized.shape[1] == 1:\n",
    "        # Handle binary classification case by adding the inverse\n",
    "        all_labels_binarized = np.hstack((1 - all_labels_binarized, all_labels_binarized))\n",
    "\n",
    "    # Compute ROC AUC\n",
    "    try:\n",
    "        if num_classes_model == 2:\n",
    "            # Binary classification: compute ROC AUC for the positive class\n",
    "            auc_roc = roc_auc_score(all_labels_binarized[:, 1], all_probs_np[:, 1])\n",
    "        else:\n",
    "            # Multi-class classification\n",
    "            auc_roc = roc_auc_score(all_labels_binarized, all_probs_np, average='macro', multi_class='ovr')\n",
    "    except ValueError:\n",
    "        auc_roc = float('nan')  # Handle cases where ROC AUC cannot be computed\n",
    "\n",
    "    # Compute F1 Score\n",
    "    f1 = f1_score(all_labels_np, all_preds_np, average='macro')\n",
    "\n",
    "    return history, all_labels_np, all_preds_np, all_probs_np, auc_roc, f1\n",
    "\n",
    "\n",
    "\n",
    "def compute_channel_stats(X_tensor):\n",
    "    \"\"\"\n",
    "    Compute per-channel mean and standard deviation.\n",
    "\n",
    "    Args:\n",
    "        X_tensor (torch.Tensor): Input tensor of shape (N, C, H, W).\n",
    "\n",
    "    Returns:\n",
    "        means (torch.Tensor): Mean for each channel.\n",
    "        stds (torch.Tensor): Standard deviation for each channel.\n",
    "    \"\"\"\n",
    "    # Compute per-channel mean and std\n",
    "    means = X_tensor.mean(dim=(0, 2, 3))\n",
    "    stds = X_tensor.std(dim=(0, 2, 3))\n",
    "    return means, stds\n",
    "\n",
    "\n",
    "class FITSDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the FITSDataset.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): Input images tensor of shape (N, C, H, W).\n",
    "            y (torch.Tensor): Labels tensor of shape (N,).\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.X[idx]  # (5, H, W) tensor\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        label = self.y[idx]\n",
    "        return img, label\n",
    "\n",
    "\n",
    "def plot_combined_metrics(fpr, tpr, roc_auc, cm, classes, model_name, fold, model_color, save_dir=\"plots\"):\n",
    "    \"\"\"\n",
    "    Plot and save the ROC curve on the left and the Confusion Matrix on the right.\n",
    "    Also displays the figure inline.\n",
    "\n",
    "    Args:\n",
    "        fpr (dict or np.ndarray): False positive rates for each class or for binary classification.\n",
    "        tpr (dict or np.ndarray): True positive rates for each class or for binary classification.\n",
    "        roc_auc (dict or float): AUC scores for each class or for binary classification.\n",
    "        cm (np.ndarray): Confusion matrix.\n",
    "        classes (list or np.ndarray): List of class labels.\n",
    "        model_name (str): Name of the model.\n",
    "        fold (int): Fold number.\n",
    "        model_color (str): Color assigned to the model.\n",
    "        save_dir (str): Directory to save the plot.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from itertools import cycle\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Create a figure with two subplots side by side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # === Left Subplot: ROC Curve ===\n",
    "    ax1 = axes[0]\n",
    "    \n",
    "    # Determine if the task is binary or multi-class\n",
    "    is_binary = False\n",
    "    if isinstance(roc_auc, float) and len(classes) == 2:\n",
    "        is_binary = True\n",
    "    elif isinstance(roc_auc, dict) and len(classes) == 2:\n",
    "        # Sometimes roc_auc can be a dict even in binary classification\n",
    "        # We'll compute the macro average and plot only the positive class\n",
    "        is_binary = True\n",
    "    \n",
    "    if is_binary:\n",
    "        # Binary classification: plot ROC for the positive class only\n",
    "        # Assuming class 1 is the positive class\n",
    "        positive_class = 1\n",
    "        if isinstance(roc_auc, float):\n",
    "            # If roc_auc is a float, it's already the AUC for the positive class\n",
    "            ax1.plot(fpr, tpr, color=model_color,\n",
    "                     lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "        elif isinstance(roc_auc, dict):\n",
    "            # If roc_auc is a dict, extract the AUC for the positive class\n",
    "            if positive_class in roc_auc:\n",
    "                ax1.plot(fpr[positive_class], tpr[positive_class], color=model_color,\n",
    "                         lw=2, label=f'ROC curve (AUC = {roc_auc[positive_class]:.2f})')\n",
    "            else:\n",
    "                print(f\"Positive class {positive_class} not found in roc_auc dict.\")\n",
    "        \n",
    "        ax1.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        ax1.set_xlim([-0.05, 1.05])\n",
    "        ax1.set_ylim([-0.05, 1.05])\n",
    "        ax1.set_xlabel('False Positive Rate')\n",
    "        ax1.set_ylabel('True Positive Rate')\n",
    "        ax1.set_title(f'ROC Curve for {model_name} Fold {fold+1}', color='black')\n",
    "        ax1.legend(loc=\"lower right\")\n",
    "    else:\n",
    "        # Multi-class classification: plot ROC for each class using model_color and different line styles\n",
    "        line_styles = cycle(['-', '--', '-.', ':', '-', '--', '-.', ':', '-'])\n",
    "        for i, ls in zip(range(len(classes)), line_styles):\n",
    "            if i not in fpr or i not in tpr or i not in roc_auc:\n",
    "                continue  # Skip classes without ROC data\n",
    "            ax1.plot(fpr[i], tpr[i], color=model_color, linestyle=ls, lw=2,\n",
    "                     label=f'ROC curve of class {classes[i]} (AUC = {roc_auc[i]:0.2f})')\n",
    "        ax1.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        ax1.set_xlim([-0.05, 1.05])\n",
    "        ax1.set_ylim([-0.05, 1.05])\n",
    "        ax1.set_xlabel('False Positive Rate')\n",
    "        ax1.set_ylabel('True Positive Rate')\n",
    "        ax1.set_title(f'ROC Curves for {model_name} Fold {fold+1}', color='black')\n",
    "        ax1.legend(loc=\"lower right\")\n",
    "    \n",
    "    # === Right Subplot: Confusion Matrix ===\n",
    "    ax2 = axes[1]\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=classes, yticklabels=classes, ax=ax2)\n",
    "    ax2.set_ylabel('Actual')\n",
    "    ax2.set_xlabel('Predicted')\n",
    "    ax2.set_title(f'Confusion Matrix for {model_name} Fold {fold+1}', color='black')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    save_path = os.path.join(save_dir, f'combined_metrics_{model_name}_fold_{fold+1}.png')\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"Combined metrics saved at {save_path}\")\n",
    "    \n",
    "    # Display the figure inline (for Jupyter Notebooks)\n",
    "    display(fig)\n",
    "    \n",
    "    # Close the figure to free memory\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "def plot_learning_curves(history, model_name, fold, model_color, save_dir=\"plots\"):\n",
    "    \"\"\"\n",
    "    Plot and save the learning curves. Also display them inline.\n",
    "\n",
    "    Args:\n",
    "        history (dict): Dictionary containing training and validation loss and accuracy.\n",
    "        model_name (str): Name of the model.\n",
    "        fold (int): Fold number.\n",
    "        model_color (str): Color assigned to the model.\n",
    "        save_dir (str): Directory to save the plot.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12,5))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history['train_loss'], 'o-', color='grey', label='Training loss')\n",
    "    plt.plot(epochs, history['val_loss'], 's-', color=model_color, label='Validation loss')\n",
    "    plt.title(f'Training and Validation Loss for {model_name} Fold {fold+1}', color='black')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history['train_acc'], 'o-', color='grey', label='Training accuracy')\n",
    "    plt.plot(epochs, history['val_acc'], 's-', color=model_color, label='Validation accuracy')\n",
    "    plt.title(f'Training and Validation Accuracy for {model_name} Fold {fold+1}', color='black')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    save_path = os.path.join(save_dir, f'learning_curves_{model_name}_fold_{fold+1}.png')\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"Learning curves saved at {save_path}\")\n",
    "    \n",
    "    # Display the figure inline\n",
    "    display(plt.gcf())\n",
    "    \n",
    "    # Close the figure to free memory\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_aggregated_roc_curves(model_roc_data, num_classes, save_dir=\"plots\"):\n",
    "    \"\"\"\n",
    "    Plot aggregated ROC curves for all models, handling binary and multi-class scenarios.\n",
    "    For binary classification, plots one ROC curve per model.\n",
    "    For multi-class classification, plots ROC curves per class for each model.\n",
    "\n",
    "    Args:\n",
    "        model_roc_data (dict): Dictionary where keys are model names and values are lists of (fpr_dict, tpr_dict) tuples per fold.\n",
    "        num_classes (int): Number of classes.\n",
    "        save_dir (str): Directory to save the plot.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import auc\n",
    "    import os\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Define colors for each model\n",
    "    colors = {\n",
    "        'ViT': 'crimson',\n",
    "        'MLP-Mixer': 'tomato',\n",
    "        'CvT': 'darkred',\n",
    "        'Swin': 'indigo',\n",
    "        'CaiT': 'royalblue',\n",
    "        'DeiT': 'cadetblue',\n",
    "        'DeiT3': 'dodgerblue',\n",
    "        'Twins_SVT': 'lightgreen',\n",
    "        'Twins_PCPVT': 'mediumseagreen',\n",
    "        'PiT': 'dimgrey',\n",
    "        'Ensemble': 'darkgoldenrod',\n",
    "        'Random': 'black'\n",
    "    }\n",
    "\n",
    "    for model_name, roc_list in model_roc_data.items():\n",
    "        if not roc_list:\n",
    "            continue  # Skip models with no ROC data\n",
    "\n",
    "        # Determine if the task is binary\n",
    "        is_binary = (num_classes == 2)\n",
    "\n",
    "        if is_binary:\n",
    "            # Binary classification: plot one ROC curve per model by averaging TPRs across folds\n",
    "            mean_fpr = np.linspace(0, 1, 100)\n",
    "            tprs = []\n",
    "            aucs = []\n",
    "\n",
    "            for fpr_dict, tpr_dict in roc_list:\n",
    "                # Assuming class 1 is the positive class\n",
    "                if 1 not in fpr_dict or 1 not in tpr_dict:\n",
    "                    continue  # Skip if positive class ROC data is missing\n",
    "                interp_tpr = np.interp(mean_fpr, fpr_dict[1], tpr_dict[1])\n",
    "                interp_tpr[0] = 0.0  # Ensure TPR starts at 0\n",
    "                tprs.append(interp_tpr)\n",
    "                aucs.append(auc(fpr_dict[1], tpr_dict[1]))\n",
    "\n",
    "            if tprs:\n",
    "                mean_tpr = np.mean(tprs, axis=0)\n",
    "                mean_tpr[-1] = 1.0  # Ensure TPR ends at 1\n",
    "                mean_auc = auc(mean_fpr, mean_tpr)\n",
    "                plt.plot(mean_fpr, mean_tpr, color=colors.get(model_name, 'black'),\n",
    "                         lw=2, label=f'{model_name} (AUC = {mean_auc:.2f})')\n",
    "        else:\n",
    "            # Multi-class classification: plot ROC curves per class for each model\n",
    "            mean_fpr = np.linspace(0, 1, 100)\n",
    "            mean_tprs = {i: [] for i in range(num_classes)}\n",
    "            mean_aucs = {i: [] for i in range(num_classes)}\n",
    "\n",
    "            for fpr_dict, tpr_dict in roc_list:\n",
    "                for i in range(num_classes):\n",
    "                    if i not in fpr_dict or i not in tpr_dict:\n",
    "                        continue  # Skip classes without ROC data\n",
    "                    interp_tpr = np.interp(mean_fpr, fpr_dict[i], tpr_dict[i])\n",
    "                    interp_tpr[0] = 0.0  # Ensure TPR starts at 0\n",
    "                    mean_tprs[i].append(interp_tpr)\n",
    "                    mean_aucs[i].append(auc(fpr_dict[i], tpr_dict[i]))\n",
    "\n",
    "            # Plot each class's aggregated ROC curve\n",
    "            for i in range(num_classes):\n",
    "                if not mean_tprs[i]:\n",
    "                    continue  # Skip if no data for this class\n",
    "                mean_tpr = np.mean(mean_tprs[i], axis=0)\n",
    "                mean_tpr[-1] = 1.0  # Ensure TPR ends at 1\n",
    "                mean_auc = auc(mean_fpr, mean_tpr)\n",
    "                plt.plot(mean_fpr, mean_tpr, color=colors.get(model_name, 'black'),\n",
    "                         lw=2, label=f'{model_name} Class {i} (AUC = {mean_auc:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    if is_binary:\n",
    "        plt.title('Aggregated ROC Curves for All Models (Binary Classification)')\n",
    "    else:\n",
    "        plt.title('Aggregated ROC Curves for All Models (Per Class)')\n",
    "    plt.legend(loc=\"lower right\", fontsize='small')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    save_path = os.path.join(save_dir, 'aggregated_roc_curves.png')\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"Aggregated ROC curves saved at {save_path}\")\n",
    "\n",
    "    # Display the figure inline (for Jupyter Notebooks)\n",
    "    plt.show()\n",
    "\n",
    "    # Close the figure to free memory\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    images_dir = \"/home/parlange/datasets/mini_sample/images\"\n",
    "    metadata_file = \"/home/parlange/datasets/mini_sample/metadata_catalog.fits\"\n",
    "\n",
    "    bands = ['g', 'r', 'i', 'z', 'y']\n",
    "    image_files = {\n",
    "        'g': 'image_catalog_g.fits',\n",
    "        'r': 'image_catalog_r.fits',\n",
    "        'i': 'image_catalog_i.fits',\n",
    "        'z': 'image_catalog_z.fits',\n",
    "        'y': 'image_catalog_y.fits'\n",
    "    }\n",
    "\n",
    "    band_data = []\n",
    "    shapes = []\n",
    "    for b in bands:\n",
    "        file_path = os.path.join(images_dir, image_files[b])\n",
    "        with fits.open(file_path) as hdul:\n",
    "            data = hdul[0].data\n",
    "            print(f\"Shape for {b} band: {data.shape}\")\n",
    "            # Replace NaN and Inf with zeros\n",
    "            data = np.nan_to_num(data, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            band_data.append(data)\n",
    "            shapes.append(data.shape)\n",
    "\n",
    "    unique_shapes = set(shapes)\n",
    "    if len(unique_shapes) != 1:\n",
    "        raise ValueError(f\"Inconsistent shapes among bands: {unique_shapes}\")\n",
    "\n",
    "    X = np.stack(band_data, axis=1)\n",
    "    print(\"Final X shape:\", X.shape)\n",
    "\n",
    "    with fits.open(metadata_file) as hdul:\n",
    "        meta_data = hdul[1].data\n",
    "        y = np.array(meta_data['label'])\n",
    "        print(\"y shape:\", y.shape)\n",
    "\n",
    "    y = y.astype(np.int64)\n",
    "\n",
    "    X_tensor = torch.from_numpy(X).float()   # (N, 5, H, W)\n",
    "    y_tensor = torch.from_numpy(y).long()    # (N,)\n",
    "\n",
    "    # Check for remaining NaNs or Infs\n",
    "    if torch.isnan(X_tensor).any():\n",
    "        print(\"X_tensor contains NaN values. Replacing with 0.\")\n",
    "        X_tensor = torch.nan_to_num(X_tensor, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    if torch.isinf(X_tensor).any():\n",
    "        print(\"X_tensor contains Inf values. Replacing with 0.\")\n",
    "        X_tensor = torch.nan_to_num(X_tensor, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    # Compute channel statistics\n",
    "    means, stds = compute_channel_stats(X_tensor)\n",
    "    print(\"Per-channel means:\", means)\n",
    "    print(\"Per-channel stds:\", stds)\n",
    "\n",
    "    # Check for NaN or Inf in means and stds\n",
    "    if torch.isnan(means).any() or torch.isnan(stds).any():\n",
    "        print(\"Computed means or stds contain NaN. Replacing with 0.5 and 0.5 respectively.\")\n",
    "        means = torch.where(torch.isnan(means), torch.tensor(0.5), means)\n",
    "        stds = torch.where(torch.isnan(stds), torch.tensor(0.5), stds)\n",
    "    if torch.isinf(means).any() or torch.isinf(stds).any():\n",
    "        print(\"Computed means or stds contain Inf. Replacing with 1.0 and 1.0 respectively.\")\n",
    "        means = torch.where(torch.isinf(means), torch.tensor(1.0), means)\n",
    "        stds = torch.where(torch.isinf(stds), torch.tensor(1.0), stds)\n",
    "\n",
    "    # Define training and validation transforms with normalization\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        RandomApply([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=30),\n",
    "            transforms.RandomResizedCrop(size=224, scale=(0.8, 1.2)),\n",
    "            transforms.RandomPerspective(distortion_scale=0.4, p=0.5),\n",
    "            transforms.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 2.0)),\n",
    "        ], p=0.5),\n",
    "        transforms.Normalize(mean=means.tolist(),\n",
    "                             std=stds.tolist())\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Normalize(mean=means.tolist(),\n",
    "                             std=stds.tolist())\n",
    "    ])\n",
    "\n",
    "    dataset = FITSDataset(X_tensor, y_tensor)\n",
    "    N = len(dataset)\n",
    "    num_classes = len(torch.unique(y_tensor))\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}')\n",
    "    if torch.cuda.is_available():\n",
    "        print(f'Number of GPUs: {torch.cuda.device_count()}')\n",
    "\n",
    "    # List of models to train\n",
    "    # Uncomment and modify as needed\n",
    "    # model_names = ['ViT', 'CaiT', 'DeiT', 'DeiT3', 'Swin', 'Twins_SVT', 'Twins_PCPVT', 'PiT', 'MLP-Mixer']\n",
    "    model_names = ['ViT', 'DeiT', 'DeiT3', 'Swin', 'Twins_SVT', 'Twins_PCPVT', 'PiT','MLP-Mixer']\n",
    "    #model_names = ['Swin']\n",
    "\n",
    "    # Define per-model fine-tuning configurations with 'last_n_blocks' mode and exact last_n values\n",
    "    #model_config = {\n",
    "    #    'ViT': {'fine_tune_mode': 'last_n_blocks', 'last_n': 6},          # 12 blocks / 2 = 6\n",
    "    #    'CaiT': {'fine_tune_mode': 'last_n_blocks', 'last_n': 6},        # 24 blocks / 2 = 12\n",
    "    #    'DeiT': {'fine_tune_mode': 'last_n_blocks', 'last_n': 6},         # 12 blocks / 2 = 6\n",
    "    #    'DeiT3': {'fine_tune_mode': 'last_n_blocks', 'last_n': 6},        # 12 blocks / 2 = 6\n",
    "    #    'Swin': {'fine_tune_mode': 'last_n_blocks', 'last_n': 6},         # 12 blocks / 2 = 6\n",
    "    #    'Twins_SVT': {'fine_tune_mode': 'last_n_blocks', 'last_n': 6},    # 12 blocks / 2 = 6\n",
    "    #    'Twins_PCPVT': {'fine_tune_mode': 'last_n_blocks', 'last_n': 2},  # 4 blocks / 2 = 2\n",
    "    #    'PiT': {'fine_tune_mode': 'last_n_blocks', 'last_n': 6},          # 12 blocks / 2 = 6\n",
    "    #    'MLP-Mixer': {'fine_tune_mode': 'last_n_blocks', 'last_n': 6},    # 12 blocks / 2 = 6\n",
    "    #}\n",
    "\n",
    "    model_config = {\n",
    "        'ViT': {'fine_tune_mode': 'classification_head', 'last_n': None},\n",
    "        'CaiT': {'fine_tune_mode': 'classification_head', 'last_n': None},\n",
    "        'DeiT': {'fine_tune_mode': 'classification_head', 'last_n': None},\n",
    "        'DeiT3': {'fine_tune_mode': 'classification_head', 'last_n': None},\n",
    "        'Swin': {'fine_tune_mode': 'classification_head', 'last_n': None},\n",
    "        'Twins_SVT': {'fine_tune_mode': 'classification_head', 'last_n': None},  # Added comma here\n",
    "        'Twins_PCPVT': {'fine_tune_mode': 'classification_head', 'last_n': None},\n",
    "        'PiT': {'fine_tune_mode': 'classification_head', 'last_n': None},\n",
    "        'MLP-Mixer': {'fine_tune_mode': 'classification_head', 'last_n': None},\n",
    "    }\n",
    "\n",
    "    k = 5\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    # To store overall results\n",
    "    model_metrics = {model: {'auc_roc': [], 'f1': []} for model in model_names}\n",
    "    # To store ROC data for aggregated plotting\n",
    "    model_roc_data = {model: [] for model in model_names}\n",
    "\n",
    "    # Define your colors dictionary\n",
    "    colors = {\n",
    "        'ViT': 'crimson',\n",
    "        'MLP-Mixer': 'tomato',\n",
    "        'CvT': 'darkred',\n",
    "        'Swin': 'indigo',\n",
    "        'CaiT': 'royalblue',\n",
    "        'DeiT': 'cadetblue',\n",
    "        'DeiT3': 'dodgerblue',\n",
    "        'Twins_SVT': 'lightgreen',\n",
    "        'Twins_PCPVT': 'mediumseagreen',\n",
    "        'PiT': 'dimgrey',\n",
    "        'Ensemble': 'darkgoldenrod',\n",
    "        'Random': 'black'\n",
    "    }\n",
    "\n",
    "    for model_name in model_names:\n",
    "        print(f\"\\n==== Training Model: {model_name} ====\\n\")\n",
    "        fold_results = []\n",
    "        # Iterate through each fold\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(np.arange(N), y_tensor.cpu().numpy())):\n",
    "            print(f\"\\n=== Fold {fold+1}/{k} for model {model_name} ===\")\n",
    "            print(f\"Train indices length: {len(train_idx)}, Val indices length: {len(val_idx)}\")\n",
    "\n",
    "            # Split dataset\n",
    "            train_subset = Subset(FITSDataset(X_tensor[train_idx], y_tensor[train_idx], transform=train_transform), range(len(train_idx)))\n",
    "            val_subset = Subset(FITSDataset(X_tensor[val_idx], y_tensor[val_idx], transform=val_transform), range(len(val_idx)))\n",
    "\n",
    "            # DataLoader settings\n",
    "            pin_memory = torch.cuda.is_available()\n",
    "            train_loader = DataLoader(train_subset, batch_size=128, shuffle=True, num_workers=4, pin_memory=pin_memory)\n",
    "            val_loader = DataLoader(val_subset, batch_size=128, shuffle=False, num_workers=4, pin_memory=pin_memory)\n",
    "\n",
    "            # Get model configuration\n",
    "            config = model_config.get(model_name, {'fine_tune_mode': 'classification_head', 'last_n': None})\n",
    "            fine_tune_mode = config['fine_tune_mode']\n",
    "            last_n = config['last_n'] if config['last_n'] is not None else 6  # Default to 6 if not set\n",
    "\n",
    "            # Print the fine-tuning mode and last_n if applicable\n",
    "            if fine_tune_mode == 'last_n_blocks':\n",
    "                print(f\"Fine-tuning mode: {fine_tune_mode} with last_n={last_n}\")\n",
    "            else:\n",
    "                print(f\"Fine-tuning mode: {fine_tune_mode}\")\n",
    "\n",
    "            # Initialize the model\n",
    "            model = initialize_model(model_name, num_classes, pretrained=False, fine_tune_mode=fine_tune_mode, last_n=last_n)\n",
    "\n",
    "            model = model.to(device)\n",
    "            if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "                model = nn.DataParallel(model)\n",
    "                print(\"Model wrapped in DataParallel.\")\n",
    "\n",
    "            try:\n",
    "                history, all_labels_np, all_preds_np, all_preds_probs, auc_roc, f1 = train_one_fold(\n",
    "                    model, train_loader, val_loader, device, num_epochs=2, patience=1, fold=fold, model_name=model_name\n",
    "                )\n",
    "                fold_results.append((auc_roc, f1))\n",
    "                model_metrics[model_name]['auc_roc'].append(auc_roc)\n",
    "                model_metrics[model_name]['f1'].append(f1)\n",
    "\n",
    "                # Retrieve the model's color from the colors dictionary\n",
    "                model_color = colors.get(model_name, 'black')\n",
    "\n",
    "                # Plot learning curves\n",
    "                plot_learning_curves(history, model_name, fold, model_color=model_color)\n",
    "\n",
    "                # Compute confusion matrix\n",
    "                cm = confusion_matrix(all_labels_np, all_preds_np)\n",
    "\n",
    "                # Compute ROC curves\n",
    "                # Binarize the labels for ROC\n",
    "                all_labels_binarized = label_binarize(all_labels_np, classes=np.arange(num_classes))\n",
    "                if all_labels_binarized.shape[1] == 1:\n",
    "                    # Handle binary classification case\n",
    "                    all_labels_binarized = np.hstack((1 - all_labels_binarized, all_labels_binarized))\n",
    "\n",
    "                fpr = dict()\n",
    "                tpr = dict()\n",
    "                roc_auc_dict = dict()\n",
    "                for i in range(num_classes):\n",
    "                    # Check if there are both positive and negative samples for the class\n",
    "                    if np.sum(all_labels_binarized[:, i]) == 0 or np.sum(1 - all_labels_binarized[:, i]) == 0:\n",
    "                        print(f\"Cannot compute ROC for class {i} in model {model_name} fold {fold+1} due to lack of positive or negative samples.\")\n",
    "                        continue\n",
    "                    fpr[i], tpr[i], _ = roc_curve(all_labels_binarized[:, i], all_preds_probs[:, i])\n",
    "                    roc_auc_dict[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "                # Store ROC data for aggregated plotting\n",
    "                model_roc_data[model_name].append((fpr, tpr))\n",
    "\n",
    "                # Plot combined metrics (ROC and Confusion Matrix)\n",
    "                plot_combined_metrics(fpr, tpr, roc_auc_dict, cm, classes=np.arange(num_classes), \n",
    "                                      model_name=model_name, fold=fold, model_color=model_color)\n",
    "\n",
    "            except ValueError as e:\n",
    "                print(f\"Training stopped due to error: {e}\")\n",
    "                # Optionally, continue with next fold or model\n",
    "                continue  # Proceed to next fold\n",
    "\n",
    "        if fold_results:\n",
    "            avg_auc = np.nanmean([res[0] for res in fold_results])\n",
    "            avg_f1 = np.nanmean([res[1] for res in fold_results])\n",
    "            print(f\"\\nResults for {model_name}: Average AUC-ROC: {avg_auc:.4f}, Average F1 Score: {avg_f1:.4f}\")\n",
    "        else:\n",
    "            print(f\"\\nNo results for {model_name} due to training issues.\")\n",
    "\n",
    "    # After all models have been trained, create and display the rankings\n",
    "    ranking_auc = sorted(model_metrics.items(), key=lambda x: np.nanmean(x[1]['auc_roc']), reverse=True)\n",
    "    ranking_f1 = sorted(model_metrics.items(), key=lambda x: np.nanmean(x[1]['f1']), reverse=True)\n",
    "\n",
    "    # Create DataFrames for rankings\n",
    "    ranking_auc_df = pd.DataFrame(ranking_auc, columns=['Model', 'Metrics'])\n",
    "    ranking_auc_df['Average AUC-ROC'] = ranking_auc_df['Metrics'].apply(lambda x: np.nanmean(x['auc_roc']))\n",
    "    ranking_auc_df = ranking_auc_df.drop('Metrics', axis=1).sort_values(by='Average AUC-ROC', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    ranking_f1_df = pd.DataFrame(ranking_f1, columns=['Model', 'Metrics'])\n",
    "    ranking_f1_df['Average F1 Score'] = ranking_f1_df['Metrics'].apply(lambda x: np.nanmean(x['f1']))\n",
    "    ranking_f1_df = ranking_f1_df.drop('Metrics', axis=1).sort_values(by='Average F1 Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Display the rankings in the notebook\n",
    "    print(\"\\n==== Model Ranking Based on AUC-ROC ====\")\n",
    "    display(ranking_auc_df)\n",
    "\n",
    "    print(\"\\n==== Model Ranking Based on F1 Score ====\")\n",
    "    display(ranking_f1_df)\n",
    "\n",
    "    # Plot aggregated ROC curves for all models\n",
    "    print(\"\\n==== Plotting Aggregated ROC Curves for All Models ====\")\n",
    "    plot_aggregated_roc_curves(model_roc_data, num_classes, save_dir=\"plots\")\n",
    "\n",
    "    # Optionally, save the ranking results to a file\n",
    "    with open(\"model_ranking.txt\", \"w\") as f:\n",
    "        f.write(\"Model Ranking Based on AUC-ROC:\\n\")\n",
    "        for rank, (model, metrics) in enumerate(ranking_auc, start=1):\n",
    "            f.write(f\"{rank}. {model} - Average AUC-ROC: {np.nanmean(metrics['auc_roc']):.4f}\\n\")\n",
    "        \n",
    "        f.write(\"\\nModel Ranking Based on F1 Score:\\n\")\n",
    "        for rank, (model, metrics) in enumerate(ranking_f1, start=1):\n",
    "            f.write(f\"{rank}. {model} - Average F1 Score: {np.nanmean(metrics['f1']):.4f}\\n\")\n",
    "    \n",
    "    print(\"\\nModel rankings have been saved to 'model_ranking.txt'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d15486-430f-459b-aea4-07e2c33fda95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "vit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
